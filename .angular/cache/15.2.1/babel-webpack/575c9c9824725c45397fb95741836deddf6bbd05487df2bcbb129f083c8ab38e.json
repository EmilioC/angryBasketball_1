{"ast":null,"code":"import _asyncToGenerator from \"C:/Users/cordo/VisualCode/appProject/angular/chatGPT_1-2/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport { Configuration, OpenAIApi } from 'openai';\nimport { environment } from '../environments/environment';\nimport { gptModels } from '../models/constants';\nimport * as i0 from \"@angular/core\";\nexport class CustomerSupportComponent {\n  constructor() {\n    this.chatConversation = [];\n    this.gptModels = gptModels;\n    this.promptText = '';\n    this.showSpinner = false;\n  }\n  ngOnInit() {}\n  checkResponse() {\n    this.pushChatContent(this.promptText, 'You', 'person');\n    this.invokeGPT();\n  }\n  pushChatContent(content, person, cssClass) {\n    const chatToPush = {\n      person: person,\n      response: content,\n      cssClass: cssClass\n    };\n    this.chatConversation.push(chatToPush);\n  }\n  getText(data) {\n    console.log(data);\n    return data.split('\\n').filter(f => f.length > 0);\n  }\n  invokeGPT() {\n    var _this = this;\n    return _asyncToGenerator(function* () {\n      if (_this.promptText.length < 2) return;\n      try {\n        _this.response = undefined;\n        let configuration = new Configuration({\n          apiKey: environment.apiKey\n        });\n        let openai = new OpenAIApi(configuration);\n        let requestData = {\n          model: 'text-davinci-003',\n          prompt: _this.promptText,\n          temperature: 0.95,\n          max_tokens: 150,\n          top_p: 1.0,\n          frequency_penalty: 0.0,\n          presence_penalty: 0.0\n        };\n        _this.showSpinner = true;\n        let apiResponse = yield openai.createCompletion(requestData);\n        _this.response = apiResponse.data;\n        _this.pushChatContent(_this.response.choices[0].text.trim(), 'Mr Bot', 'bot');\n        debugger;\n        _this.showSpinner = false;\n      } catch (error) {\n        _this.showSpinner = false;\n        // Consider adjusting the error handling logic for your use case\n        if (error.response) {\n          console.error(error.response.status, error.response.data);\n        } else {\n          console.error(`Error with OpenAI API request: ${error.message}`);\n        }\n      }\n    })();\n  }\n}\nCustomerSupportComponent.ɵfac = function CustomerSupportComponent_Factory(t) {\n  return new (t || CustomerSupportComponent)();\n};\nCustomerSupportComponent.ɵcmp = /*@__PURE__*/i0.ɵɵdefineComponent({\n  type: CustomerSupportComponent,\n  selectors: [[\"app-customer-support\"]],\n  decls: 2,\n  vars: 0,\n  template: function CustomerSupportComponent_Template(rf, ctx) {\n    if (rf & 1) {\n      i0.ɵɵelementStart(0, \"p\");\n      i0.ɵɵtext(1, \"customer-support works!\");\n      i0.ɵɵelementEnd();\n    }\n  },\n  encapsulation: 2\n});","map":{"version":3,"mappings":";AACA,SAASA,aAAa,EAAEC,SAAS,QAAQ,QAAQ;AACjD,SAASC,WAAW,QAAQ,6BAA6B;AACzD,SAASC,SAAS,QAAQ,qBAAqB;;AAO/C,OAAM,MAAOC,wBAAwB;EAOnCC;IANF,qBAAgB,GAAgB,EAAE;IAE9B,cAAS,GAAGF,SAAS;IACrB,eAAU,GAAG,EAAE;IACf,gBAAW,GAAG,KAAK;EAEL;EAEhBG,QAAQ,IACR;EAEAC,aAAa;IACX,IAAI,CAACC,eAAe,CAAC,IAAI,CAACC,UAAU,EAAC,KAAK,EAAC,QAAQ,CAAC;IACpD,IAAI,CAACC,SAAS,EAAE;EAClB;EAGAF,eAAe,CAACG,OAAc,EAAEC,MAAa,EAAEC,QAAe;IAC5D,MAAMC,UAAU,GAAgB;MAAEF,MAAM,EAACA,MAAM;MAAEG,QAAQ,EAACJ,OAAO;MAAEE,QAAQ,EAACA;IAAQ,CAAC;IACrF,IAAI,CAACG,gBAAgB,CAACC,IAAI,CAACH,UAAU,CAAC;EACxC;EAGAI,OAAO,CAACC,IAAW;IACjBC,OAAO,CAACC,GAAG,CAACF,IAAI,CAAC;IACjB,OAAOA,IAAI,CAACG,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAACC,CAAC,IAAEA,CAAC,CAACC,MAAM,GAAC,CAAC,CAAC;EAE/C;EAEMf,SAAS;IAAA;IAAA;MAGb,IAAG,KAAI,CAACD,UAAU,CAACgB,MAAM,GAAC,CAAC,EAC3B;MAIA,IAAG;QACD,KAAI,CAACV,QAAQ,GAAGW,SAAS;QACzB,IAAIC,aAAa,GAAG,IAAI3B,aAAa,CAAC;UAAC4B,MAAM,EAAE1B,WAAW,CAAC0B;QAAM,CAAC,CAAC;QACnE,IAAIC,MAAM,GAAG,IAAI5B,SAAS,CAAC0B,aAAa,CAAC;QAEzC,IAAIG,WAAW,GAAC;UACdC,KAAK,EAAE,kBAAkB;UACzBC,MAAM,EAAE,KAAI,CAACvB,UAAU;UACvBwB,WAAW,EAAE,IAAI;UACjBC,UAAU,EAAE,GAAG;UACfC,KAAK,EAAE,GAAG;UACVC,iBAAiB,EAAE,GAAG;UACtBC,gBAAgB,EAAE;SACnB;QACD,KAAI,CAACC,WAAW,GAAG,IAAI;QACvB,IAAIC,WAAW,SAAUV,MAAM,CAACW,gBAAgB,CAACV,WAAW,CAAC;QAE7D,KAAI,CAACf,QAAQ,GAAGwB,WAAW,CAACpB,IAAqB;QACjD,KAAI,CAACX,eAAe,CAAC,KAAI,CAACO,QAAQ,CAAC0B,OAAO,CAAC,CAAC,CAAC,CAACC,IAAI,CAACC,IAAI,EAAE,EAAC,QAAQ,EAAC,KAAK,CAAC;QAC/E;QACM,KAAI,CAACL,WAAW,GAAG,KAAK;OACzB,QAAMM,KAAS,EAAE;QAChB,KAAI,CAACN,WAAW,GAAG,KAAK;QACxB;QACA,IAAIM,KAAK,CAAC7B,QAAQ,EAAE;UAClBK,OAAO,CAACwB,KAAK,CAACA,KAAK,CAAC7B,QAAQ,CAAC8B,MAAM,EAAED,KAAK,CAAC7B,QAAQ,CAACI,IAAI,CAAC;SAE1D,MAAM;UACLC,OAAO,CAACwB,KAAK,CAAC,kCAAkCA,KAAK,CAACE,OAAO,EAAE,CAAC;;;IAGnE;EACH;;AAtEW1C,wBAAwB;mBAAxBA,wBAAwB;AAAA;AAAxBA,wBAAwB;QAAxBA,wBAAwB;EAAA2C;EAAAC;EAAAC;EAAAC;IAAA;MCVrCC,yBAAG;MAAAA,uCAAuB;MAAAA,iBAAI","names":["Configuration","OpenAIApi","environment","gptModels","CustomerSupportComponent","constructor","ngOnInit","checkResponse","pushChatContent","promptText","invokeGPT","content","person","cssClass","chatToPush","response","chatConversation","push","getText","data","console","log","split","filter","f","length","undefined","configuration","apiKey","openai","requestData","model","prompt","temperature","max_tokens","top_p","frequency_penalty","presence_penalty","showSpinner","apiResponse","createCompletion","choices","text","trim","error","status","message","selectors","decls","vars","template","i0"],"sourceRoot":"","sources":["C:\\Users\\cordo\\VisualCode\\appProject\\angular\\chatGPT_1-2\\src\\app\\customer-support\\customer-support.component.ts","C:\\Users\\cordo\\VisualCode\\appProject\\angular\\chatGPT_1-2\\src\\app\\customer-support\\customer-support.component.html"],"sourcesContent":["import { Component, OnInit } from '@angular/core';\nimport { Configuration, OpenAIApi } from 'openai';\nimport { environment } from '../environments/environment';\nimport { gptModels } from '../models/constants';\nimport { ChatWithBot, ResponseModel } from '../models/gpt-response';\n\n@Component({\n  selector: 'app-customer-support',\n  templateUrl: './customer-support.component.html'\n})\nexport class CustomerSupportComponent implements OnInit {\nchatConversation: ChatWithBot[]=[];\nresponse!: ResponseModel | undefined;\n    gptModels = gptModels\n    promptText = '';\n    showSpinner = false;\n\n  constructor() { }\n\n  ngOnInit(): void {\n  }\n\n  checkResponse() {\n    this.pushChatContent(this.promptText,'You','person');\n    this.invokeGPT();\n  }\n\n\n  pushChatContent(content:string, person:string, cssClass:string) {\n    const chatToPush: ChatWithBot = { person:person, response:content, cssClass:cssClass};\n    this.chatConversation.push(chatToPush);\n  }\n\n\n  getText(data:string) {\n    console.log(data);\n    return data.split('\\n').filter(f=>f.length>0);\n    \n  }\n\n  async invokeGPT() {\n   \n\n    if(this.promptText.length<2)\n    return;\n\n    \n\n    try{\n      this.response = undefined;\n      let configuration = new Configuration({apiKey: environment.apiKey});\n      let openai = new OpenAIApi(configuration);\n\n      let requestData={\n        model: 'text-davinci-003',//'text-davinci-003',//\"text-curie-001\",\n        prompt: this.promptText,//this.generatePrompt(animal),\n        temperature: 0.95,\n        max_tokens: 150,\n        top_p: 1.0,\n        frequency_penalty: 0.0,\n        presence_penalty: 0.0,\n      };\n      this.showSpinner = true;\n      let apiResponse =  await openai.createCompletion(requestData);\n\n      this.response = apiResponse.data as ResponseModel;\n      this.pushChatContent(this.response.choices[0].text.trim(),'Mr Bot','bot');\ndebugger;\n      this.showSpinner = false;\n    }catch(error:any) {\n      this.showSpinner = false;\n      // Consider adjusting the error handling logic for your use case\n      if (error.response) {\n        console.error(error.response.status, error.response.data);\n        \n      } else {\n        console.error(`Error with OpenAI API request: ${error.message}`);\n        \n      }\n    }\n  }\n}\n","<p>customer-support works!</p>\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}